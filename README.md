# Not all reviews are created equal?

This project contains exploratory scripts for sentiment analysis on Amazon Review Dataset 2023.

> **Does the sentiment expressed in a review causally affect its helpfulness, as measured by helpful votes?**

---

## Table of Contents

1. [Overview](#overview)
2. [Dataset](#dataset)
3. [Sentiment Analysis](#sentiment-analysis)
4. [Quick Start](#quick-start)
5. [File Layout](#file-layout)
6. [Dependencies](#dependencies)

---

## Overview

1. **Samples** an equal number of raw reviews per Amazon category from the [McAuley‑Lab/Amazon‑Reviews‑2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023) HuggingFace dataset.
2. **Cleans** the text (lower‑cases, trims, drops very short reviews, counts tokens).
3. **Scores sentiment** with two approaches

   * **VADER** (`nltk` lexicon)
   * **Multilingual BERT** (`nlptown/bert-base-multilingual-uncased-sentiment`)
4. Writes each intermediate dataset to CSV so you can inspect / reuse any stage.
5. Casual Inference on several datasets

By changing two flags you can benchmark VADER vs BERT latency on your own machine.

---

## Dataset

The [McAuley‑Lab/Amazon‑Reviews‑2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023) is a large-scale Amazon Reviews dataset.
This dataset of product reviews covers
a wide variety of product categories and provides
insights into customer opinions and behaviors on
Amazon.
Below is an aggregated summary of the dataset:

- Total Number of Reviews - 571.4M
- Total Number of Users - 240.8M
- Average Review Length (tokens) - 52.7
- Total categories - 34 (33 named categories and an "Unknown" category)

Since the entire dataset is pretty large, we randomly sample it.
The `dataset.py` defines function to generate and clean dataset
of different sizes. 

### Generating a sample of the dataset 
From each category the script selects a random sample equal to that specified by the `--category-size` argument.
It defaults to `25000`

### Cleaning the Reviews
The cleaning process is surmised below:

- Keeps only string reviews  
- Lower‑cases & strips
- Drops reviews ≤ 5 words<br>
- Adds `review_length`(total words in review) & `token_count` (total tokens in review)


```shell
python main.py --category-size=25000
```

## Sentiment Analysis

After preprocessing each text review we run sentiment analysis on the model.
We generate a `"negative"`, `"neurtal"`, and `"positive"` label for each review.

We use two techniques to generate these labels

1. VADER - We use `nltk`'s implementation of VADER sentiment analysis as a baseline.
2. BERT - For more accurate results we use a BERT finetuned for sentiment analysis, specifically [nlptown/bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment). We assume it is a good fit, since it was trained on product reviews. We collapsed the star-rating system into a simpler "positive/neutral/negative" label for our purpose.

Due to compute constraints we could only generate sentiment analysis of the following `category_size`
- 10000  
- 25000 
- 50000 
- 100000 (VADER only)
- 1000000 (VADER only) 

`disagreement.py` script lets you see differences in the labels generated by the two models.

#### Dataset (Category Size = 25000, N=429041)

#### Class proportions

| Sentiment |    VADER   |    BERT    |
| --------- | :--------: | :--------: |
| negative  | **0.1150** | **0.1487** |
| neutral   |   0.0803   |   0.0995   |
| positive  |   0.8047   |   0.7518   |


#### VADER × BERT cross‑tab (counts)

| VADER \ BERT | negative | neutral | positive |
| ------------ | -------: | ------: | -------: |
| **negative** |   30 314 |   8 391 |   10 646 |
| **neutral**  |   12 476 |   5 650 |   16 312 |
| **positive** |   21 008 |  28 644 |  295 600 |

Cohen κ = 0.385

#### Dataset (Category Size = 50000, N=865190)

#### Class proportions

| Sentiment |   VADER   |    BERT   |
| --------- | :-------: | :-------: |
| negative  | **0.116** | **0.149** |
| neutral   |   0.082   |   0.100   |
| positive  |   0.802   |   0.751   |

#### VADER × BERT cross‑tab (counts)

| VADER \ BERT |   negative | neutral |    positive |
| ------------ | ---------: | ------: | ----------: |
| **negative** | **60 721** |  17 309 |      22 145 |
| **neutral**  |     25 639 |  11 783 |      33 285 |
| **positive** |     42 134 |  57 595 | **594 579** |

Cohen κ = 0.384

### Overall

Both models label most reviews as positive, but VADER labels slightly more as positive. 
80 % positives for VADER versus 75 % for BERT.


## Quick Start

```bash
# 1. Set up a fresh venv (optional)
python -m venv .venv
source .venv/bin/activate   # or .venv\Scripts\activate on Windows

# 2. Install deps
pip install -r requirements.txt
#   (transformers, datasets, nltk, pandas, tqdm, torch, etc.)

# 3. Run the pipeline with default (25000reviews / category)
python main.py
```

## File Layout (default `batch_size = 25000`)

```bash
reviews-25000.csv                     # raw sample, all 34 categories
reviews-25000-cleaned.csv             # lowercase, >5 words, token counts
reviews-25000-analysis-vader.csv      # + VADER sentiment
reviews-25000-analysis-bert.csv       # + BERT sentiment
```

---

## Dependencies

```
pandas
datasets>=2.0
transformers>=4.34
torch          # CUDA build recommended for GPU
tqdm
nltk
```

```python
# one‑time download for VADER
import nltk; nltk.download("vader_lexicon")
```

---

### License

MIT — do whatever you want, but cite the *McAuley‑Lab/Amazon‑Reviews‑2023* dataset if you publish results.